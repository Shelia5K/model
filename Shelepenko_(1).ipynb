{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5sEMEO9q0AbA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#path_to_file= tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "path_to_file =\"./les1.txt\"\n",
        "text=open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text:{len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD1n5MtkrCoz",
        "outputId": "6123567a-807f-456b-8589-a12273a7bc75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text:10164 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZY2blgxeaq9",
        "outputId": "05491707-40bb-4db2-e269-13903d1f3365"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лесь Подерв’янський.\r\n",
            "Павлік Морозов.\r\n",
            "Епічна трагедія.\r\n",
            "Дійові особи\r\n",
            "Павлiк Морозов, атлетичний юнак, нордична краса i гiтлерюгендiвська зачiска; одягнут просто i зi смаком: в бiлу сорочку i короткi шкiрянi штанцi; з рiзних бокiв Павлiк Морозов пер\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEVNuro_LZzi",
        "outputId": "9e45ad47-6121-407f-d9c8-86e46e921813"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = tf.strings.unicode_split(path_to_file, input_encoding='UTF-8')\n"
      ],
      "metadata": {
        "id": "ra9_8szwqmj1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars=tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "chars_from_ids =tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "id": "AnfHFU5xL0Hf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05dcae5c-0bce-4d9b-db4b-a33a524f1415"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=string, numpy=\n",
              "array([b'.', b'[UNK]', b'l', b'e', b'[UNK]', b'[UNK]', b'.', b'[UNK]',\n",
              "       b'[UNK]', b'[UNK]'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "id": "Tp1nbGVOe_H9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1419803-7643-40d7-f368-ebd403dc6cfa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([ 9,  0, 17, 15,  0,  0,  9,  0,  0,  0])>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "pcnKpCMTZwwe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "id": "D-DEnP2GNbmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9069ef2a-d952-4456-ea28-ff55510fda09"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10164,), dtype=int64, numpy=array([34, 55, 67, ..., 69, 85,  9])>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "YOuKZ2bqOAK4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(100):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "#for ids in ids_dataset.take(40):\n",
        "    #print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "\n",
        "seq_length = 100"
      ],
      "metadata": {
        "id": "XsZwFZ8tOypI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b3623c-affd-4bd2-a04d-fc67b7da5a51"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Л\n",
            "е\n",
            "с\n",
            "ь\n",
            " \n",
            "П\n",
            "о\n",
            "д\n",
            "е\n",
            "р\n",
            "в\n",
            "’\n",
            "я\n",
            "н\n",
            "с\n",
            "ь\n",
            "к\n",
            "и\n",
            "й\n",
            ".\n",
            "\r\n",
            "\n",
            "\n",
            "П\n",
            "а\n",
            "в\n",
            "л\n",
            "і\n",
            "к\n",
            " \n",
            "М\n",
            "о\n",
            "р\n",
            "о\n",
            "з\n",
            "о\n",
            "в\n",
            ".\n",
            "\r\n",
            "\n",
            "\n",
            "Е\n",
            "п\n",
            "і\n",
            "ч\n",
            "н\n",
            "а\n",
            " \n",
            "т\n",
            "р\n",
            "а\n",
            "г\n",
            "е\n",
            "д\n",
            "і\n",
            "я\n",
            ".\n",
            "\r\n",
            "\n",
            "\n",
            "Д\n",
            "і\n",
            "й\n",
            "о\n",
            "в\n",
            "і\n",
            " \n",
            "о\n",
            "с\n",
            "о\n",
            "б\n",
            "и\n",
            "\r\n",
            "\n",
            "\n",
            "П\n",
            "а\n",
            "в\n",
            "л\n",
            "i\n",
            "к\n",
            " \n",
            "М\n",
            "о\n",
            "р\n",
            "о\n",
            "з\n",
            "о\n",
            "в\n",
            ",\n",
            " \n",
            "а\n",
            "т\n",
            "л\n",
            "е\n",
            "т\n",
            "и\n",
            "ч\n",
            "н\n",
            "и\n",
            "й\n",
            " \n",
            "ю\n",
            "н\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "#for seq in sequences.take(1):\n",
        "  #print(chars_from_ids(seq))"
      ],
      "metadata": {
        "id": "0LIX8AKNss1u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#len(sequences)\n",
        "#for seq in sequences.take(2):\n",
        "  #print(text_from_ids(seq).numpy())\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "v155j-dbt7N9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split_input_target(list(\"Tensorflow\"))\n",
        "dataset = sequences.map(split_input_target)\n",
        "dataset"
      ],
      "metadata": {
        "id": "LL3q4pk6r4Ms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80fedef-36a0-4489-d222-24321a557aef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "unQQQGoVvDPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e767b1de-3994-4fb0-abd5-f21833e0264b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'\\xd0\\x9b\\xd0\\xb5\\xd1\\x81\\xd1\\x8c \\xd0\\x9f\\xd0\\xbe\\xd0\\xb4\\xd0\\xb5\\xd1\\x80\\xd0\\xb2\\xe2\\x80\\x99\\xd1\\x8f\\xd0\\xbd\\xd1\\x81\\xd1\\x8c\\xd0\\xba\\xd0\\xb8\\xd0\\xb9.\\r\\n\\xd0\\x9f\\xd0\\xb0\\xd0\\xb2\\xd0\\xbb\\xd1\\x96\\xd0\\xba \\xd0\\x9c\\xd0\\xbe\\xd1\\x80\\xd0\\xbe\\xd0\\xb7\\xd0\\xbe\\xd0\\xb2.\\r\\n\\xd0\\x95\\xd0\\xbf\\xd1\\x96\\xd1\\x87\\xd0\\xbd\\xd0\\xb0 \\xd1\\x82\\xd1\\x80\\xd0\\xb0\\xd0\\xb3\\xd0\\xb5\\xd0\\xb4\\xd1\\x96\\xd1\\x8f.\\r\\n\\xd0\\x94\\xd1\\x96\\xd0\\xb9\\xd0\\xbe\\xd0\\xb2\\xd1\\x96 \\xd0\\xbe\\xd1\\x81\\xd0\\xbe\\xd0\\xb1\\xd0\\xb8\\r\\n\\xd0\\x9f\\xd0\\xb0\\xd0\\xb2\\xd0\\xbbi\\xd0\\xba \\xd0\\x9c\\xd0\\xbe\\xd1\\x80\\xd0\\xbe\\xd0\\xb7\\xd0\\xbe\\xd0\\xb2, \\xd0\\xb0\\xd1\\x82\\xd0\\xbb\\xd0\\xb5\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xbd\\xd0\\xb8\\xd0\\xb9 \\xd1\\x8e\\xd0\\xbd'\n",
            "Target: b'\\xd0\\xb5\\xd1\\x81\\xd1\\x8c \\xd0\\x9f\\xd0\\xbe\\xd0\\xb4\\xd0\\xb5\\xd1\\x80\\xd0\\xb2\\xe2\\x80\\x99\\xd1\\x8f\\xd0\\xbd\\xd1\\x81\\xd1\\x8c\\xd0\\xba\\xd0\\xb8\\xd0\\xb9.\\r\\n\\xd0\\x9f\\xd0\\xb0\\xd0\\xb2\\xd0\\xbb\\xd1\\x96\\xd0\\xba \\xd0\\x9c\\xd0\\xbe\\xd1\\x80\\xd0\\xbe\\xd0\\xb7\\xd0\\xbe\\xd0\\xb2.\\r\\n\\xd0\\x95\\xd0\\xbf\\xd1\\x96\\xd1\\x87\\xd0\\xbd\\xd0\\xb0 \\xd1\\x82\\xd1\\x80\\xd0\\xb0\\xd0\\xb3\\xd0\\xb5\\xd0\\xb4\\xd1\\x96\\xd1\\x8f.\\r\\n\\xd0\\x94\\xd1\\x96\\xd0\\xb9\\xd0\\xbe\\xd0\\xb2\\xd1\\x96 \\xd0\\xbe\\xd1\\x81\\xd0\\xbe\\xd0\\xb1\\xd0\\xb8\\r\\n\\xd0\\x9f\\xd0\\xb0\\xd0\\xb2\\xd0\\xbbi\\xd0\\xba \\xd0\\x9c\\xd0\\xbe\\xd1\\x80\\xd0\\xbe\\xd0\\xb7\\xd0\\xbe\\xd0\\xb2, \\xd0\\xb0\\xd1\\x82\\xd0\\xbb\\xd0\\xb5\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xbd\\xd0\\xb8\\xd0\\xb9 \\xd1\\x8e\\xd0\\xbd\\xd0\\xb0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "qg00IKrewtij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74c7f30-797a-41bc-bab7-7e8ad0cebe4c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "print(vocab_size)\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n"
      ],
      "metadata": {
        "id": "GZ5HdqxIwi-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568a8501-4b69-4efe-c441-d252504e3771"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, return_sequences=True)))\n",
        "model.add(tf.keras.layers.Dense(vocab_size, activation=\"softmax\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "HOgxgeVww_Fz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "id": "BGsL4V05xwiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d7ba86-5ce8-4f1c-cecf-addfb3f2237b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 85) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "HXisyNAHz6nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cccf9db2-4f59-4c52-fea0-191521cbc34f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 256)         21760     \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, None, 2048)       10493952  \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 85)          174165    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,689,877\n",
            "Trainable params: 10,689,877\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "metadata": {
        "id": "hdgPZmIt0FEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4e73cb-afbe-42b6-c2ea-82e00320f976"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([66,  3, 42, 77, 42, 78, 24, 25, 53, 67, 84, 66, 51, 24, 78, 81,  5,\n",
              "        5, 14, 58, 38, 29, 48, 12, 75, 40, 39, 45, 59,  4, 77, 63, 17, 84,\n",
              "       69, 49, 39,  9, 76, 28, 74,  1, 20, 27, 63, 74,  2, 39, 37, 83, 11,\n",
              "       10,  4,  9, 77,  3, 17, 42, 18, 64, 60, 21, 52, 24, 61, 48, 42, 65,\n",
              "       22, 48, 46, 28, 72, 36, 55, 84, 65, 53, 77, 18, 55, 81, 80, 63, 67,\n",
              "       77,  7, 31, 65, 39, 17, 17, 40, 47, 34, 30, 11, 34, 35, 43])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy().decode('utf-8'))\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "1RZIXQqx13oG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "949f6a64-a922-4d66-fb22-415cfd96f2b3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " iзку, якiй зовсiм недавно признавався в любвi. Берiзка ламається.\r\n",
            "\r\n",
            "Учитель атеїзму (до дiтей).\r\n",
            "Дє\n",
            "\n",
            "Next Char Predictions:\n",
            " р УюУяАБгс“рбАяї((bиПЕЩ?щСРЦй!юнl“уЯР.ьДш\n",
            "pГнш\rРО’;:!.ю lУmокrвАлЩУпІЩЧДцНе“пгюmеїінсю,ЗпРllСШЛЖ;ЛМФ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the model**"
      ],
      "metadata": {
        "id": "WT7n6Va6My-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adam=tf.keras.optimizers.Adam() #(learning_rate=0.01)\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(loss=loss, optimizer=adam) # metrics=['accuracy'])\n",
        "#history=model.fit(xs, ys, epochs =100, verbose =1)"
      ],
      "metadata": {
        "id": "jMwX4ChQNB0q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "id": "KV3n9bHsOa-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53062473-4d82-4d38-9f19-0b774836adac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 85)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.4417105, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "id": "NFpHD0hcOhTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e89a1fe-61d1-4072-98c7-ffbf2e49b705"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84.920074"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "KpoY8bgcPYvS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100"
      ],
      "metadata": {
        "id": "Qecx3lZ7Plyy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "bbREwEFMPnwy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4025d8ff-29b1-41ff-a191-4b1f039bc352"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 26s 26s/step - loss: 4.4417\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 4.3958\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 4.1059\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 12s 12s/step - loss: 7.1727\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 14s 14s/step - loss: 3.9633\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.9037\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 4.0397\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 4.0378\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 12s 12s/step - loss: 3.9796\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.9265\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 12s 12s/step - loss: 3.8167\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 14s 14s/step - loss: 3.7176\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.6381\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.5381\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.5036\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.5311\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.5228\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.5282\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.5318\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.5076\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.5038\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.5056\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.4725\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.4558\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.4500\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.4429\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.4533\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.4398\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.4284\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.4330\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.4304\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.4044\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.4153\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.4033\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 14s 14s/step - loss: 3.3992\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.3837\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.3740\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.3572\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.3601\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.3329\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.3308\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.3165\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.2919\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.2977\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.2886\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.2533\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.2432\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.2124\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 15s 15s/step - loss: 3.2104\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 14s 14s/step - loss: 3.1711\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 15s 15s/step - loss: 3.1323\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.1025\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.0685\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.0767\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 3.0094\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.9558\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.9243\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.8500\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.7894\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.7551\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.7076\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.6529\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 14s 14s/step - loss: 2.6077\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.6020\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.6283\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.5547\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.4560\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.3931\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.3403\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.2768\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.2443\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.1673\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.0952\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.0731\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 2.0065\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.9750\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.9400\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.8722\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.8129\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.7610\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.6847\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.6263\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.5342\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.4827\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.4287\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 15s 15s/step - loss: 1.3769\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.3295\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.3091\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.2306\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.1651\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.1049\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.0665\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.0348\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 0.9715\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 0.9225\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 0.8894\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 0.8434\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 0.7993\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 0.7610\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 0.7219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "ypvaV-RlR5wC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "l3h9bZWCR7WK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#start = time.time()\n",
        "#states = None\n",
        "#next_char = tf.constant(['Павлiк Морозов:'])\n",
        "#result = [next_char]\n",
        "\n",
        "#for n in range(2000):\n",
        "  #next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  #result.append(next_char)\n",
        "\n",
        "#result = tf.strings.join(result)\n",
        "#end = time.time()\n",
        "#print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "#print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "RyvD8CukR--i"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"./les1.h5\")"
      ],
      "metadata": {
        "id": "UU0HLSQYTFjh"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs"
      ],
      "metadata": {
        "id": "4VullMpKTfDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a854027c-cfb9-4b38-9942-8a5e935e9fb8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.10/dist-packages (4.6.0)\n",
            "Requirement already satisfied: flax<0.6.3,>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.6.2)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (5.12.0)\n",
            "Requirement already satisfied: jax>=0.3.16 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.10)\n",
            "Requirement already satisfied: tensorflow<3,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-decision-forests>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.13.0)\n",
            "Requirement already satisfied: packaging~=20.9 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (20.9)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (3.7.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (1.0.5)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (0.1.5)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (0.1.36)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (13.3.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (6.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.16->tensorflowjs) (0.1.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.16->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.16->tensorflowjs) (1.10.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging~=20.9->tensorflowjs) (3.0.9)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (3.8.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (16.0.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (0.32.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.3.0->tensorflowjs) (1.5.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.3.0->tensorflowjs) (0.40.0)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.3.0->tensorflowjs) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax<0.6.3,>=0.6.2->tensorflowjs) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax<0.6.3,>=0.6.2->tensorflowjs) (2.14.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax->flax<0.6.3,>=0.6.2->tensorflowjs) (0.1.7)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax->flax<0.6.3,>=0.6.2->tensorflowjs) (0.4.10+cuda11.cudnn86)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.3.0->tensorflowjs) (2022.7.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax<0.6.3,>=0.6.2->tensorflowjs) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax<0.6.3,>=0.6.2->tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1->flax<0.6.3,>=0.6.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter --input_format=keras \"./les1.h5\" ./"
      ],
      "metadata": {
        "id": "_lZgoHmRTg6B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25a491d-d8ce-4185-ee4a-9fe10102e84c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-04 13:54:12.010915: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-04 13:54:15.639034: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    }
  ]
}